{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRvqAOMU5qr9"
      },
      "source": [
        "# Problem 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QB4DymD5v3Z"
      },
      "source": [
        "a) Instantiate a sequence to sequence transformer (self-attention) using a standard API (tensorflow, pytorch, etc). Verify with some examples that it is permutation equivariant.\n",
        "\n",
        "b) Prove analytically using the defining equations of self-attention that the transformer architecture is permutation equivariant.\n",
        "\n",
        "c) Repeat (b) for multi-headed attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev966SuTVME8"
      },
      "source": [
        "# Problem 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h1DnK6UVPLm"
      },
      "source": [
        "a) Train the MNIST VAE from class. Illustrate with some example scans across the 2d latent space how the digits morph from one into another.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we've built a VAE from scratch (I chose to do this so I could learn what was going on under the hood). First, let's instantiate the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Import MNIST dataset\n",
        "\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "mnist_data = datasets.MNIST(root='data', train=True, \n",
        "                            transform = T.Compose(\n",
        "                                        [T.ToTensor(), \n",
        "                                        T.Lambda(lambda x: x.view(-1))]\n",
        "                                        ),\n",
        "                            download=False)\n",
        "\n",
        "# Parameters for training\n",
        "batch_size = 100\n",
        "batch_size_test = 100\n",
        "num_epochs = 10\n",
        "\n",
        "\n",
        "# Try using dataloader module to randomly provide datapts from dataset\n",
        "\n",
        "size_mnist = 28*28"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we'll architect the VAE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Architecting the variational autoencoder\n",
        "\n",
        "class VAE_MNIST(nn.Module):\n",
        "    def __init__(self, latent_space_dim): \n",
        "        super(VAE_MNIST, self).__init__()\n",
        "        \n",
        "        # define the encoder \n",
        "        layer_one_size = 500\n",
        "        layer_two_size = 200\n",
        "        layer_three_size = 20\n",
        "        \n",
        "        self.fc1 = nn.Linear(size_mnist, layer_one_size)\n",
        "        self.fc2 = nn.Linear(layer_one_size, layer_two_size)\n",
        "        self.fc3 = nn.Linear(layer_two_size, layer_three_size)\n",
        "        self.mean = nn.Linear(layer_three_size, latent_space_dim)\n",
        "        self.std_dev = nn.Linear(layer_three_size, latent_space_dim)\n",
        "        \n",
        "    \n",
        "        # define decoder layers \n",
        "        \n",
        "        self.dfc1 = nn.Linear(latent_space_dim, layer_three_size)\n",
        "        self.dfc2 = nn.Linear(layer_three_size, layer_two_size)\n",
        "        self.dfc3 = nn.Linear(layer_two_size, layer_one_size)\n",
        "        self.img = nn.Linear(layer_one_size, size_mnist)\n",
        "        \n",
        "        # Bring the image down multiple layers into the latent dimensional space\n",
        "        \n",
        "    def encoder(self, x): \n",
        "        \n",
        "        \n",
        "        h = F.leaky_relu(self.fc1(x))\n",
        "        h = F.leaky_relu(self.fc2(h))\n",
        "        h = self.fc3(h)\n",
        "        return self.mean(h), self.std_dev(h)\n",
        "\n",
        "    \n",
        "    # Performs the reparameterization trick\n",
        "    \n",
        "    def sampling(self, mu, log_variance):\n",
        "        std = torch.exp(0.5*log_variance)\n",
        "        epsilon = torch.randn_like(std)\n",
        "        return mu + std*epsilon # double check if this is allowed \n",
        "    \n",
        "    # Bring point in latent dimensional space to image space \n",
        "    \n",
        "    def decoder(self, x): \n",
        "        h = F.leaky_relu(self.dfc1(x))\n",
        "        h = F.leaky_relu(self.dfc2(h))\n",
        "        h = F.leaky_relu(self.dfc3(h))\n",
        "        \n",
        "        return torch.sigmoid(self.img(h))\n",
        "\n",
        "    # Forward step \n",
        "    \n",
        "    def forward(self, x): \n",
        "        # encode in latent space\n",
        "        mu, log_variance = self.encoder(x)\n",
        "        # sample from the distribution\n",
        "        z = self.sampling(mu, log_variance)\n",
        "        # decode \n",
        "        return self.decoder(z), mu, log_variance\n",
        "    \n",
        "    def loss(self, output, input, mu, log_variance):\n",
        "        # Since the prior is just the normal dist. the KL divergence takes on the form \n",
        "        KL = -1/2*torch.sum(1+log_variance-mu.pow(2)-log_variance.exp())\n",
        "        # Binary cross-entropy is same as usual\n",
        "        BCE = F.binary_cross_entropy(output, input, reduction='sum')\n",
        "        return BCE+KL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Subset\n",
        "\n",
        "# Define autoencoder model\n",
        "\n",
        "mnist_vae = VAE_MNIST(latent_space_dim=2)\n",
        "mnist_vae_optim = torch.optim.Adam(mnist_vae.parameters())\n",
        "\n",
        "# train the model\n",
        "\n",
        "\n",
        "def vae_train(model, epoch_size, dataset, optimizer): \n",
        "    model.train()\n",
        "    dataloader = DataLoader(dataset=Subset(mnist_data, np.random.choice(len(mnist_data), 5000)), batch_size=100, shuffle=True)\n",
        "    for epoch in range(epoch_size):\n",
        "        for input, target in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            output, mu, log_variance = model(input)\n",
        "            \n",
        "            loss = model.loss(output, input, mu, log_variance)\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vae_train(mnist_vae, 20, mnist_data, mnist_vae_optim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Map out all of the different numbers in our latent space\n",
        "\n",
        "\n",
        "# plot_dataloader = DataLoader(dataset=Subset(mnist_data, np.random.choice(len(mnist_data), 1000)), batch_size=1, shuffle=True)\n",
        "# colors = plt.cm.tab10(np.linspace(0,1,10))\n",
        "\n",
        "\n",
        "# xlist  = []\n",
        "# ylist = []\n",
        "# for input, target in plot_dataloader:\n",
        "#     result, mu, log_variance = mnist_vae(input)\n",
        "#     with torch.no_grad():\n",
        "#         plt.scatter(mu.numpy()[0][0], mu.numpy()[0][1], color = colors[target])\n",
        "\n",
        "\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "# Map out all of the different numbers in our latent space\n",
        "\n",
        "\n",
        "colors = plt.cm.tab10(np.linspace(0,1,10))\n",
        "\n",
        "\n",
        "empty = []\n",
        "list_coords = [[[],[]] for _ in range(10)]\n",
        "    \n",
        "with torch.no_grad():\n",
        "    for input, target in Subset(mnist_data, np.random.choice(len(mnist_data),5000)):\n",
        "        result, mu, log_variance = mnist_vae(input)\n",
        "        list_coords[target][0].append(mu[0])\n",
        "        list_coords[target][1].append(mu[1])\n",
        "\n",
        "    for target, element in enumerate(list_coords):\n",
        "        plt.scatter(element[0], element[1], color = colors[target], marker = '.')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now attempt to generate a number from the graph\n",
        "\n",
        "# new_image = new_image.reshape((28,28)) \n",
        "# with torch.no_grad():\n",
        "#     plt.imshow(new_image.numpy(), cmap='Greys')\n",
        "    \n",
        "# grid\n",
        "\n",
        "\n",
        "rows = 10\n",
        "cols = 10\n",
        "\n",
        "xlist = np.linspace(-4, 4, rows)\n",
        "ylist = np.linspace(-4, 4, cols)\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols)\n",
        "\n",
        "xpts, ypts = np.meshgrid(xlist, ylist)\n",
        "\n",
        "plt.plot(xpts, ypts)\n",
        "    \n",
        "    \n",
        "with torch.no_grad():\n",
        "    for i, row in enumerate(xpts): \n",
        "        for j, item in enumerate(xpts):\n",
        "            new_img = mnist_vae.decoder(torch.tensor([xpts[i][j], ypts[i][j]], dtype=torch.float))\n",
        "            new_img = new_img.reshape((28, 28))\n",
        "            im = axes[i][j].imshow(new_img.numpy(), cmap = 'Greys')\n",
        "            axes[i][j].set_xticks([])\n",
        "            axes[i][j].set_yticks([])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "b) Train a vanilla autoencoder on MNIST and compare the latent space to the VAE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare to a standard autoencoder and see the results\n",
        "\n",
        "\n",
        "class standard_autoencoder(nn.Module):\n",
        "    def __init__(self, latent_space_dim): \n",
        "        super(standard_autoencoder, self).__init__()\n",
        "        \n",
        "        # define the encoder \n",
        "        layer_one_size = 100\n",
        "        layer_two_size = 50\n",
        "        layer_three_size = 20\n",
        "        \n",
        "        self.fc1 = nn.Linear(size_mnist, layer_one_size)\n",
        "        self.fc2 = nn.Linear(layer_one_size, layer_two_size)\n",
        "        self.fc3 = nn.Linear(layer_two_size, layer_three_size)\n",
        "        self.mean = nn.Linear(layer_three_size, latent_space_dim)\n",
        "        self.std_dev = nn.Linear(layer_three_size, latent_space_dim)\n",
        "        \n",
        "        \n",
        "        # define decoder layers \n",
        "        \n",
        "        self.dfc1 = nn.Linear(latent_space_dim, layer_three_size)\n",
        "        self.dfc2 = nn.Linear(layer_three_size, layer_two_size)\n",
        "        self.dfc3 = nn.Linear(layer_two_size, layer_one_size)\n",
        "        self.img = nn.Linear(layer_one_size, size_mnist)\n",
        "        \n",
        "        \n",
        "        # Bring the image down multiple layers into the latent dimensional space\n",
        "        \n",
        "    def encoder(self, x): \n",
        "        h = F.relu(self.fc1(x))\n",
        "        h = F.relu(self.fc2(h))\n",
        "        h = F.relu(self.fc3(h))\n",
        "        return self.mean(h), self.std_dev(h)\n",
        "    \n",
        "    # Bring point in latent dimensional space to image space \n",
        "    \n",
        "    def decoder(self, x): \n",
        "        h = F.relu(self.dfc1(x))\n",
        "        h = F.relu(self.dfc2(h))\n",
        "        h = F.relu(self.dfc3(h))\n",
        "        \n",
        "        return torch.sigmoid(self.img(h))\n",
        "\n",
        "    # Forward step \n",
        "    \n",
        "    def forward(self, x): \n",
        "        # encode in latent space\n",
        "        mu, log_variance = self.encoder(x)\n",
        "        # decode \n",
        "        return self.decoder(mu), mu, log_variance\n",
        "    \n",
        "    def loss(self, output, input, mu, log_variance):\n",
        "        # Just the binary cross-entropy loss\n",
        "        BCE = F.binary_cross_entropy(output, input, reduction='sum')\n",
        "        return BCE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train this normal autoencoder \n",
        "\n",
        "mnist_autoencoder = standard_autoencoder(latent_space_dim=2)\n",
        "mnist_autoencoder_optimizer = torch.optim.Adam(mnist_autoencoder.parameters())  \n",
        "\n",
        "vae_train(model=mnist_autoencoder, epoch_size=20, dataset=mnist_data, optimizer=mnist_autoencoder_optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "colors = plt.cm.tab10(np.linspace(0,1,10))\n",
        "\n",
        "empty = []\n",
        "list_coords = [[[],[]] for _ in range(10)]\n",
        "    \n",
        "with torch.no_grad():\n",
        "    for input, target in Subset(mnist_data, np.random.choice(len(mnist_data),5000)):\n",
        "        result, mu, log_variance = mnist_autoencoder(input)\n",
        "        list_coords[target][0].append(mu[0])\n",
        "        list_coords[target][1].append(mu[1])\n",
        "\n",
        "    for target, element in enumerate(list_coords):\n",
        "        plt.scatter(element[0], element[1], color = colors[target], marker = '.')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot one of those cool diagrams\n",
        "\n",
        "\n",
        "rows = 15\n",
        "cols = 15\n",
        "\n",
        "xlist = np.linspace(-10, 10, rows)\n",
        "ylist = np.linspace(-10, 20, cols)\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols)\n",
        "\n",
        "xpts, ypts = np.meshgrid(xlist, ylist)\n",
        "\n",
        "plt.plot(xpts, ypts)\n",
        "    \n",
        "    \n",
        "with torch.no_grad():\n",
        "    for i, row in enumerate(xpts): \n",
        "        for j, item in enumerate(xpts):\n",
        "            new_img = mnist_autoencoder.decoder(torch.tensor([xpts[i][j], ypts[i][j]], dtype=torch.float))\n",
        "            new_img = new_img.reshape((28, 28))\n",
        "            im = axes[i][j].imshow(new_img.numpy(), cmap = 'Greys')\n",
        "            axes[i][j].set_xticks([])\n",
        "            axes[i][j].set_yticks([])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uRPPW7H6NYH"
      },
      "source": [
        "# Problem 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3pFss1N6Pcp"
      },
      "source": [
        "a) Train a vanilla GAN, WGAN and variational autoencoder on MNIST data. You can use the examples provided in class. Generate samples from each and train a binary classifier on each vs the reference data. What AUC scores do you get?\n",
        "\n",
        "b) Train a log posterior metric on all three models. Which performs best?\n",
        "\n",
        "c) Train a supervised classifier on MNIST and use this to diagnose mode collapse in the three generative models. How do they fare?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "2jGfsyO15n2Y"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module): \n",
        "    def __init__(self, input_dim): \n",
        "        super(Discriminator, self).__init__()\n",
        "        \n",
        "        layer_one_size = 512\n",
        "        layer_two_size = 256\n",
        "        \n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, layer_one_size), \n",
        "            nn.ReLU(), \n",
        "            nn.Linear(layer_one_size, layer_two_size), \n",
        "            nn.ReLU(), \n",
        "            nn.Linear(layer_two_size, 1), #bring it to just one output\n",
        "        )\n",
        "    \n",
        "    def forward(self, input): \n",
        "        return self.model(input)\n",
        "\n",
        "    def loss(self, input, output): \n",
        "        return F.binary_cross_entropy(input, output)\n",
        "\n",
        "class Generator(nn.Module): \n",
        "    def __init__(self, latent_dim_size, input_dim): \n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        \n",
        "        layer_one_size = 64\n",
        "        layer_two_size = 128\n",
        "        layer_three_size = 256\n",
        "        \n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim_size, layer_one_size), \n",
        "            nn.LeakyReLU(0.2), \n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(layer_one_size, layer_two_size), \n",
        "            nn.LeakyReLU(0.2), \n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(layer_two_size, layer_three_size), \n",
        "            nn.LeakyReLU(0.2), \n",
        "            nn.Dropout(0.3),nn.Linear(layer_three_size, input_dim), \n",
        "            nn.LeakyReLU(0.2), \n",
        "            nn.Sigmoid()  # a good idea since the MNIST data set is bound [0,1]\n",
        "        )\n",
        "        \n",
        "        self.latent_dim_size = latent_dim_size\n",
        "        \n",
        "    def forward(self, input): \n",
        "        return self.model(input)\n",
        "\n",
        "    def loss(self, input, output):\n",
        "        return F.binary_cross_entropy(input, output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we need to define the losses \n",
        "\n",
        "def train_gan(discriminator: Discriminator, generator: Generator, discriminator_optimizer, generator_optimizer, dataset, epochs): \n",
        "    # first generate set of data\n",
        "    \n",
        "    batch_size = 100\n",
        "    \n",
        "    dataloader = DataLoader(dataset=Subset(dataset, np.random.choice(len(dataset), 10000)), batch_size=batch_size, shuffle=True)\n",
        "    for epoch in range(epochs):\n",
        "        for i, real_data in enumerate(dataloader):\n",
        "            # generate elements\n",
        "            latent_space_sample = torch.randn((batch_size, generator.latent_dim_size))\n",
        "            generated_data = generator(latent_space_sample)\n",
        "            \n",
        "            data_in = real_data[0]\n",
        "            \n",
        "            real_labels = torch.ones((batch_size))\n",
        "            fake_labels = torch.zeros((batch_size))\n",
        "\n",
        "            \n",
        "            # feed to discriminator\n",
        "            discriminator.train()\n",
        "            discriminator_optimizer.zero_grad()\n",
        "            real_loss = F.binary_cross_entropy_with_logits(discriminator(data_in), real_labels.unsqueeze(1))\n",
        "            fake_loss = F.binary_cross_entropy_with_logits(discriminator(generated_data), fake_labels.unsqueeze(1))\n",
        "            discriminator_loss = real_loss+fake_loss\n",
        "            discriminator_loss.backward()\n",
        "            discriminator_optimizer.step()\n",
        "            \n",
        "            # Now, train the generator\n",
        "            \n",
        "            latent_space_sample = torch.randn((batch_size, generator.latent_dim_size))\n",
        "            \n",
        "            generator.train()\n",
        "            \n",
        "            generator_optimizer.zero_grad()\n",
        "            \n",
        "            output_discriminator_generated = discriminator(generator(latent_space_sample))\n",
        "            # test against all ones, see how it does\n",
        "            generated_loss  = F.binary_cross_entropy_with_logits(output_discriminator_generated, real_labels.unsqueeze(1))\n",
        "            \n",
        "            generated_loss.backward()\n",
        "            generator_optimizer.step()\n",
        "            \n",
        "        if epoch%5==0:\n",
        "            with torch.no_grad():\n",
        "                new_img = (generator(torch.randn((generator.latent_dim_size))).reshape((28,28)).numpy())\n",
        "                plt.imshow(new_img, cmap='Grays')\n",
        "            print(f'Current Generator Loss: {generated_loss}')\n",
        "            print(f'Current Discriminator Loss: {discriminator_loss}')\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Target size (torch.Size([100])) must be the same as input size (torch.Size([100, 1]))",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[79], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m mnist_generator_optim \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(mnist_generator\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m      5\u001b[0m mnist_discriminator_optim \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(mnist_discriminator\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmnist_discriminator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmnist_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmnist_discriminator_optim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmnist_generator_optim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmnist_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[78], line 40\u001b[0m, in \u001b[0;36mtrain_gan\u001b[1;34m(discriminator, generator, discriminator_optimizer, generator_optimizer, dataset, epochs)\u001b[0m\n\u001b[0;32m     38\u001b[0m output_discriminator_generated \u001b[38;5;241m=\u001b[39m discriminator(generator(latent_space_sample))\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# test against all ones, see how it does\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m generated_loss  \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_discriminator_generated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m generated_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     43\u001b[0m generator_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\functional.py:3624\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m-> 3624\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3625\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3626\u001b[0m     )\n\u001b[0;32m   3628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\n\u001b[0;32m   3629\u001b[0m     \u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum\n\u001b[0;32m   3630\u001b[0m )\n",
            "\u001b[1;31mValueError\u001b[0m: Target size (torch.Size([100])) must be the same as input size (torch.Size([100, 1]))"
          ]
        }
      ],
      "source": [
        "mnist_generator = Generator(32, 28*28)\n",
        "mnist_discriminator = Discriminator(28*28)\n",
        "\n",
        "mnist_generator_optim = torch.optim.Adam(mnist_generator.parameters(), lr=0.001)\n",
        "mnist_discriminator_optim = torch.optim.Adam(mnist_discriminator.parameters(), lr=0.001)\n",
        "\n",
        "train_gan(mnist_discriminator, mnist_generator, mnist_discriminator_optim, mnist_generator_optim, mnist_data, 200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
