{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "mean_1 = np.ones(10)*0.5\n",
    "mean_2 = np.zeros(10) \n",
    "num_samples = 150\n",
    "num_test=100\n",
    "\n",
    "COV = np.diag(np.ones(10))\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_1 = np.random.multivariate_normal(mean=mean_1,cov=COV,size=num_samples)\n",
    "sample_2=np.random.multivariate_normal(mean=mean_2,cov=COV,size=num_samples)\n",
    "\n",
    "test_1 = np.random.multivariate_normal(mean=mean_1,cov=COV,size=num_test)\n",
    "test_2 = np.random.multivariate_normal(mean=mean_2,cov=COV,size=num_test)\n",
    "\n",
    "# Now that we have our two datasets, let's \"mix them up\" and assign the relavant classes\n",
    "# This is our x vector \n",
    "training_data = np.concatenate((sample_1,sample_2))\n",
    "\n",
    "# Labels will be 0 for dataset 1, 1 for dataset 2\n",
    "# This is basically our y vector\n",
    "training_data_labels = np.concatenate((np.zeros(num_samples), np.ones(num_samples)))\n",
    "\n",
    "\n",
    "test_data = np.concatenate((test_1,test_2))\n",
    "\n",
    "test_labels = np.concatenate((np.zeros(num_test), np.ones(num_test)))\n",
    "\n",
    "# Produce the sigmoid function\n",
    "\n",
    "def sigma(a):\n",
    "    return 1/(1+np.exp(a))\n",
    "\n",
    "\n",
    "# vec_mu is a function of vec_w and our datapoints \n",
    "\n",
    "def cross_entropy(p,q):\n",
    "    return -(p*np.log(q)+(1-p)*np.log(1-q))\n",
    "\n",
    "def NLL(vec_w, bias, vec_y, mtx_X, l):\n",
    "    to_ret = 0\n",
    "    N, C = np.array(mtx_X).shape\n",
    "\n",
    "    # Modify to absorb bias \n",
    "    mtx_X = [np.concatenate((x,[1])) for x in mtx_X]\n",
    "    for n in range(N):\n",
    "        mu_n = sigma(vec_w @ mtx_X[n]+bias)\n",
    "        to_ret+=cross_entropy(vec_y[n],mu_n)\n",
    "    return  1/N*to_ret+l*np.dot(vec_w,vec_w)\n",
    "\n",
    "\n",
    "def grad_NLL(vec_w, vec_y, mtx_X, l):\n",
    "    N, C = np.array(mtx_X).shape\n",
    "    # Modify to absorb bias \n",
    "    mtx_X= [np.concatenate((x, [1])) for x in mtx_X]\n",
    "\n",
    "    vec_mu = [sigma(x) for x in mtx_X @ vec_w]\n",
    "    return (np.transpose(np.ones(N)) @ np.diag(np.array(vec_mu)-np.array(vec_y)) @ mtx_X)/N+2*l*vec_w\n",
    "\n",
    "\n",
    "# With our grad created, we can now perform grad descent\n",
    "\n",
    "# We know L(theta) to be our negative log likelihood fn we're trying to mimimize, so let's create a basic SGD algorithm\n",
    "\n",
    "def SGD(vec_y, mtx_X, num_iterations, step_size,l):\n",
    "    N, C = np.array(mtx_X).shape\n",
    "    # Create a theta with weights and biases incorporated\n",
    "    theta = np.concatenate((np.ones(C),[1]))\n",
    "    random_sample_size = 10\n",
    "    for k in range(num_iterations):\n",
    "        # Get a random sample of data points \n",
    "        random_samples = [np.random.randint(0,N) for i in range(random_sample_size)]\n",
    "        # Add a vector onto theta \n",
    "        vec_y_subset = [vec_y[random_samples[i]] for i in range(random_sample_size)]\n",
    "        mtx_X_subset = [mtx_X[random_samples[i]] for i in range(random_sample_size)]\n",
    "\n",
    "        theta = theta-step_size*grad_NLL(theta, vec_y_subset,mtx_X_subset,l)\n",
    "    return theta\n",
    "\n",
    "def model(sample_vector, weights):\n",
    "    # Allow for bias\n",
    "    sample_vector = np.concatenate((sample_vector,[1]))\n",
    "    return sigma(np.dot(weights,sample_vector))\n",
    "\n",
    "def provide_guess(sample_vector, weights):\n",
    "    probability = model(sample_vector=sample_vector, weights=weights)\n",
    "    return 1 if np.random.uniform(0,1)>probability else 0\n",
    "\n",
    "def calculate_accuracy(data, data_labels, weights):\n",
    "    confusion_matrix = [[0,0],[0,0]]\n",
    "    N=len(data)\n",
    "    guesses = np.zeros(N)\n",
    "    for i in range(N): \n",
    "        guesses[i] = provide_guess(sample_vector=data[i], weights=weights)\n",
    "        if (guesses[i]==data_labels[i]):\n",
    "            if guesses[i]==0:\n",
    "                confusion_matrix[0][0]+=1\n",
    "            if guesses[i]==1:\n",
    "                confusion_matrix[1][1]+=1\n",
    "        else:\n",
    "            if guesses[i]==1:\n",
    "                confusion_matrix[0][1]+=1\n",
    "            if guesses[i]==0:\n",
    "                confusion_matrix[1][0]+=1\n",
    "    return confusion_matrix, (confusion_matrix[0][0]+confusion_matrix[1][1])/N\n",
    "\n",
    "# Create a grid of points (here we'll make a 2D grid)\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "y = np.linspace(-10, 10, 1000)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Create a 2D array to hold the results of the function\n",
    "Z = np.zeros_like(X)\n",
    "\n",
    "# Now, calculate ensemble of models (in this case 100 diff models)\n",
    "\n",
    "l_values = np.linspace(0,10,1000)\n",
    "\n",
    "ensemble_models = [SGD(vec_y=training_data_labels,mtx_X=training_data, num_iterations=1000,step_size=0.01, l=l_index) for l_index in l_values]\n",
    "\n",
    "accuracy = [calculate_accuracy(data=test_data,data_labels=test_labels,weights=model) for model in ensemble_models]\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.log(l_values),[x[1] for x in accuracy],'o', markersize=3)\n",
    "plt.title(\"Accuracy of model vs lambda\")\n",
    "plt.xlabel(\"log(lambda)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "weights = SGD(vec_y=training_data_labels,mtx_X=training_data,num_iterations=1000,step_size=2,l=0.2)\n",
    "\n",
    "lambda_max = np.argmax([x[1] for x in accuracy])\n",
    "print(l_values[lambda_max])\n",
    "\n",
    "# Loop through the grid and apply model to each vector (x, y)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        Z[i, j] = model([X[i, j], Y[i, j]], ensemble_models[lambda_max])  # Pass each grid point as a vector\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Scatter plot and decision boundary using MAP Logistic Regression\")\n",
    "x_1,y_1=zip(*sample_1)\n",
    "x_2,y_2=zip(*sample_2)\n",
    "\n",
    "plt.plot(x_1,y_1,'o', markersize=3)\n",
    "plt.plot(x_2,y_2,'x',markersize=3)\n",
    "\n",
    "plt.imshow(Z,extent=[-10, 10, -10, 10], origin='lower', cmap='coolwarm', alpha=0.5)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
